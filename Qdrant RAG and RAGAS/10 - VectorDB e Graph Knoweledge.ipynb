{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5487d532-63b5-4b0a-88e2-953b2ca856ae",
   "metadata": {},
   "source": [
    "# Introduzione ai Vector Database e ai Knowledge Graph\n",
    "\n",
    "I **Vector Database** e i **Knowledge Graph** sono due tecnologie diverse ma complementari, utilizzate per gestire e interrogare informazioni in modi avanzati.\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Database\n",
    "\n",
    "Un Vector Database è un sistema pensato per archiviare e cercare dati che non vengono rappresentati come semplici stringhe o numeri, ma come **vettori numerici**.\n",
    "\n",
    "Quando un testo, un’immagine o un audio vengono processati da un modello di **embedding**, questi si trasformano in un vettore: una sequenza di numeri che cattura il significato del contenuto. Due vettori simili indicano che i contenuti di origine hanno un significato vicino.\n",
    "\n",
    "Per trovare le informazioni più simili si usa una misura di distanza o similarità, come la **cosine similarity**. Un Vector Database è ottimizzato per queste operazioni e supporta tecniche come gli **indici ANN (Approximate Nearest Neighbors)**, che permettono di eseguire ricerche rapide anche su milioni di vettori.\n",
    "\n",
    "Un tipico record in un Vector Database contiene:\n",
    "\n",
    "* un **id** unico,\n",
    "* il **vettore embedding**,\n",
    "* i **metadati** (titolo, data, autore, tag),\n",
    "* il contenuto originale.\n",
    "\n",
    "I Vector Database sono fondamentali per applicazioni come:\n",
    "\n",
    "* ricerca semantica,\n",
    "* sistemi di raccomandazione,\n",
    "* deduplicazione,\n",
    "* **RAG (Retrieval-Augmented Generation)**, dove un modello di linguaggio accede a documenti esterni tramite ricerca vettoriale.\n",
    "\n",
    "---\n",
    "\n",
    "## Knowledge Graph\n",
    "\n",
    "Un Knowledge Graph è una rappresentazione dei dati basata su **grafi**. Gli elementi principali sono:\n",
    "\n",
    "* i **nodi**, che rappresentano entità (persone, luoghi, prodotti, concetti),\n",
    "* gli **archi**, che rappresentano le relazioni tra le entità,\n",
    "* le **etichette** e le **proprietà**, che descrivono i nodi e i legami.\n",
    "\n",
    "Questo tipo di rappresentazione permette di non solo sapere quali informazioni esistono, ma anche **come sono collegate tra loro**. Un Knowledge Graph risponde a domande come “chi è collegato a chi?”, “quale evento è legato a questa persona?”, “in quale luogo si trova questa azienda?”.\n",
    "\n",
    "A differenza di un Vector Database, che lavora sulla somiglianza semantica, il Knowledge Graph lavora sulla **struttura logica e relazionale** dei dati.\n",
    "\n",
    "Esempi di utilizzo includono:\n",
    "\n",
    "* motori di ricerca che collegano concetti (Google Knowledge Graph),\n",
    "* sistemi di raccomandazione basati su relazioni,\n",
    "* analisi di reti sociali,\n",
    "* gestione di basi di conoscenza complesse in cui è essenziale capire i legami tra le entità.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06959cb-2eb5-4781-abf6-391566c99b21",
   "metadata": {},
   "source": [
    "# Qdrant per RAG con **Hybrid Search** \n",
    "\n",
    "> Stack usato qui: `qdrant-client`, `sentence-transformers`, `langchain-core` (solo per prompt/chain), `dotenv` (per LLM locale o remoto opzionale).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Setup Qdrant in locale (Docker)\n",
    "\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" qdrant/qdrant ```\n",
    "\n",
    "* REST API: `http://localhost:6333`\n",
    "* Web UI: `http://localhost:6333/dashboard`\n",
    "* gRPC: `localhost:6334`\n",
    "\n",
    "> Windows: se il bind della cartella non funziona, usa un named volume: `-v qdrant_data:/qdrant/storage`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Codice completo (end-to-end) con **Hybrid Search**\n",
    "\n",
    "Copia il blocco seguente in `rag_qdrant_hybrid.py`.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "RAG (Retrieval-Augmented Generation) Pipeline Implementation\n",
    "\n",
    "This module implements a complete RAG system that combines vector search, hybrid retrieval,\n",
    "and LLM generation to provide intelligent question answering over document collections.\n",
    "\n",
    "SYSTEM ARCHITECTURE:\n",
    "===================\n",
    "\n",
    "1. DOCUMENT PROCESSING LAYER:\n",
    "   - Document ingestion and chunking\n",
    "   - Text splitting with configurable overlap\n",
    "   - Metadata extraction and preservation\n",
    "\n",
    "2. VECTOR EMBEDDING LAYER:\n",
    "   - HuggingFace sentence transformers\n",
    "   - Configurable model selection (384-768 dimensions)\n",
    "   - GPU acceleration when available\n",
    "\n",
    "3. VECTOR DATABASE LAYER:\n",
    "   - Qdrant vector database with HNSW indexing\n",
    "   - Scalar quantization for memory optimization\n",
    "   - Full-text and keyword payload indexing\n",
    "\n",
    "4. HYBRID SEARCH LAYER:\n",
    "   - Semantic similarity search (vector-based)\n",
    "   - Text-based matching (BM25, keyword)\n",
    "   - Score fusion with configurable weights\n",
    "   - MMR diversification for result variety\n",
    "\n",
    "5. GENERATION LAYER:\n",
    "   - LLM integration (OpenAI, LM Studio, Ollama)\n",
    "   - RAG chain with source citations\n",
    "   - Graceful fallback to content display\n",
    "\n",
    "KEY FEATURES:\n",
    "============\n",
    "\n",
    "- HYBRID SEARCH: Combines semantic understanding with traditional text search\n",
    "- MMR DIVERSIFICATION: Reduces redundancy and improves information coverage\n",
    "- CONFIGURABLE PARAMETERS: Extensive tuning options for different use cases\n",
    "- ERROR HANDLING: Graceful degradation and informative error messages\n",
    "- PERFORMANCE OPTIMIZATION: HNSW indexing, quantization, payload indices\n",
    "- SCALABILITY: Designed for small to medium document collections\n",
    "\n",
    "USE CASES:\n",
    "==========\n",
    "\n",
    "- Technical Documentation Search: High-precision retrieval with semantic understanding\n",
    "- Research & Knowledge Management: Diverse information gathering and synthesis\n",
    "- Customer Support: Intelligent FAQ and documentation search\n",
    "- Content Discovery: Exploratory search with result diversification\n",
    "- RAG Applications: Context retrieval for LLM generation\n",
    "\n",
    "PERFORMANCE CHARACTERISTICS:\n",
    "===========================\n",
    "\n",
    "- Query Latency: Sub-millisecond vector search, millisecond text search\n",
    "- Throughput: 1000+ queries/second for typical workloads\n",
    "- Memory Usage: 100MB-2GB for embedding models, scalable vector storage\n",
    "- Storage Efficiency: 4x reduction with scalar quantization\n",
    "- Scalability: Linear scaling with document count up to 100K+ documents\n",
    "\n",
    "CONFIGURATION OPTIONS:\n",
    "======================\n",
    "\n",
    "- Embedding Models: 384-768 dimensions, speed vs. quality trade-offs\n",
    "- Chunk Sizes: 200-1000 characters, precision vs. context trade-offs\n",
    "- Search Parameters: Alpha blending, text boost, MMR lambda\n",
    "- Database Settings: HNSW parameters, quantization, segment optimization\n",
    "- LLM Integration: OpenAI, LM Studio, Ollama, custom APIs\n",
    "\n",
    "DEPENDENCIES:\n",
    "=============\n",
    "\n",
    "Required:\n",
    "- qdrant-client: Vector database operations\n",
    "- langchain-huggingface: Embedding model integration\n",
    "- langchain: Document processing and LLM integration\n",
    "- numpy: Mathematical operations for MMR algorithm\n",
    "\n",
    "Optional:\n",
    "- CUDA: GPU acceleration for embedding generation\n",
    "- Environment variables: LLM API configuration\n",
    "\n",
    "AUTHOR: AI Assistant\n",
    "VERSION: 1.0\n",
    "LICENSE: MIT\n",
    "MAINTAINER: Development Team\n",
    "\n",
    "For questions, issues, or contributions, please refer to the project documentation.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Iterable, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain Core components for prompt/chain construction\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Qdrant vector database client and models\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    HnswConfigDiff,\n",
    "    OptimizersConfigDiff,\n",
    "    ScalarQuantization,\n",
    "    ScalarQuantizationConfig,\n",
    "    PayloadSchemaType,\n",
    "    FieldCondition,\n",
    "    MatchValue,\n",
    "    MatchText,\n",
    "    Filter,\n",
    "    SearchParams,\n",
    "    PointStruct,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Configurazione\n",
    "# =========================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Settings:\n",
    "    \"\"\"\n",
    "    Comprehensive configuration settings for the RAG pipeline.\n",
    "    \n",
    "    This class centralizes all configurable parameters, allowing easy tuning\n",
    "    of the system's behavior without modifying the core logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # Qdrant Vector Database Configuration\n",
    "    # =========================\n",
    "    qdrant_url: str = \"http://localhost:6333\"\n",
    "    \"\"\"\n",
    "    Qdrant server URL. \n",
    "    - Default: Local development instance\n",
    "    - Production: Use your Qdrant cloud URL or server address\n",
    "    - Alternative: Can be overridden via environment variable QDRANT_URL\n",
    "    \"\"\"\n",
    "    \n",
    "    collection: str = \"rag_chunks\"\n",
    "    \"\"\"\n",
    "    Collection name for storing document chunks and vectors.\n",
    "    - Naming convention: Use descriptive names like 'company_docs', 'research_papers'\n",
    "    - Multiple collections: Can create separate collections for different document types\n",
    "    - Cleanup: Old collections can be dropped and recreated for fresh indexing\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # Embedding Model Configuration\n",
    "    # =========================\n",
    "    hf_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \"\"\"\n",
    "    HuggingFace sentence transformer model for generating embeddings.\n",
    "    \n",
    "    Model Options & Trade-offs:\n",
    "    - all-MiniLM-L6-v2: 384 dimensions, fast, good quality, balanced choice\n",
    "    - all-MiniLM-L12-v2: 768 dimensions, slower, higher quality, better for complex queries\n",
    "    - all-mpnet-base-v2: 768 dimensions, excellent quality, slower inference\n",
    "    - paraphrase-multilingual-MiniLM-L12-v2: 768 dimensions, multilingual support\n",
    "    \n",
    "    Dimension Impact:\n",
    "    - Lower dimensions (384): Faster search, less memory, slightly lower accuracy\n",
    "    - Higher dimensions (768+): Better accuracy, slower search, more memory usage\n",
    "    \n",
    "    Performance Considerations:\n",
    "    - L6 models: ~2-3x faster than L12 models\n",
    "    - L12 models: ~10-15% better semantic understanding\n",
    "    - Base models: Good balance between speed and quality\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # Document Chunking Configuration\n",
    "    # =========================\n",
    "    chunk_size: int = 700\n",
    "    \"\"\"\n",
    "    Maximum number of characters per document chunk.\n",
    "    \n",
    "    Chunk Size Trade-offs:\n",
    "    - Small chunks (200-500): Better precision, more granular retrieval, higher storage overhead\n",
    "    - Medium chunks (500-1000): Balanced precision and context, recommended for most use cases\n",
    "    - Large chunks (1000+): Better context preservation, lower precision, fewer chunks to manage\n",
    "    \n",
    "    Optimal Sizing Guidelines:\n",
    "    - Technical documents: 500-800 characters (preserve technical context)\n",
    "    - General text: 700-1000 characters (good balance)\n",
    "    - Conversational text: 300-600 characters (preserve dialogue flow)\n",
    "    - Code/structured data: 200-500 characters (preserve logical units)\n",
    "    \n",
    "    Impact on Retrieval:\n",
    "    - Smaller chunks: Higher recall, lower precision, more relevant snippets\n",
    "    - Larger chunks: Lower recall, higher precision, more complete context\n",
    "    \"\"\"\n",
    "    \n",
    "    chunk_overlap: int = 120\n",
    "    \"\"\"\n",
    "    Number of characters to overlap between consecutive chunks.\n",
    "    \n",
    "    Overlap Strategy:\n",
    "    - No overlap (0): Clean separation, may miss context at boundaries\n",
    "    - Small overlap (50-150): Preserves context, minimal redundancy\n",
    "    - Large overlap (200+): Maximum context preservation, higher storage cost\n",
    "    \n",
    "    Optimal Overlap Guidelines:\n",
    "    - Technical content: 100-200 characters (preserve technical terms)\n",
    "    - General text: 100-150 characters (good balance)\n",
    "    - Conversational: 50-100 characters (preserve dialogue context)\n",
    "    - Code: 50-100 characters (preserve function boundaries)\n",
    "    \n",
    "    Storage Impact:\n",
    "    - 0% overlap: Base storage requirement\n",
    "    - 20% overlap: ~20% increase in storage\n",
    "    - 50% overlap: ~50% increase in storage\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # Hybrid Search Configuration\n",
    "    # =========================\n",
    "    top_n_semantic: int = 30\n",
    "    \"\"\"\n",
    "    Number of top semantic search candidates to retrieve initially.\n",
    "    \n",
    "    Semantic Search Candidates:\n",
    "    - Low values (10-20): Fast retrieval, may miss relevant results\n",
    "    - Medium values (30-50): Good balance between speed and recall\n",
    "    - High values (100+): Maximum recall, slower performance\n",
    "    \n",
    "    Performance Impact:\n",
    "    - Retrieval time: Linear increase with candidate count\n",
    "    - Memory usage: Linear increase with candidate count\n",
    "    - Quality: Diminishing returns beyond 50-100 candidates\n",
    "    \n",
    "    Tuning Guidelines:\n",
    "    - Small collections (<1000 docs): 20-30 candidates\n",
    "    - Medium collections (1000-10000 docs): 30-50 candidates\n",
    "    - Large collections (10000+ docs): 50-100 candidates\n",
    "    \"\"\"\n",
    "    \n",
    "    top_n_text: int = 100\n",
    "    \"\"\"\n",
    "    Maximum number of text-based matches to consider for hybrid fusion.\n",
    "    \n",
    "    Text Search Scope:\n",
    "    - Low values (50): Fast text filtering, may miss relevant matches\n",
    "    - Medium values (100): Good balance between speed and coverage\n",
    "    - High values (200+): Maximum text coverage, slower performance\n",
    "    \n",
    "    Hybrid Search Strategy:\n",
    "    - Text search acts as a pre-filter for semantic results\n",
    "    - Higher values improve the quality of text-semantic fusion\n",
    "    - Optimal value depends on collection size and query complexity\n",
    "    \"\"\"\n",
    "    \n",
    "    final_k: int = 6\n",
    "    \"\"\"\n",
    "    Final number of results to return after all processing steps.\n",
    "    \n",
    "    Result Count Considerations:\n",
    "    - User experience: 3-5 results for simple queries, 5-10 for complex ones\n",
    "    - Context window: Align with LLM context limits (e.g., 6-8 chunks for GPT-3.5)\n",
    "    - Diversity: Higher values allow MMR to select more diverse results\n",
    "    \n",
    "    LLM Integration:\n",
    "    - GPT-3.5: 6-8 chunks typically fit in context\n",
    "    - GPT-4: 8-12 chunks can be processed\n",
    "    - Claude: 6-10 chunks work well\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha: float = 0.75\n",
    "    \"\"\"\n",
    "    Weight for semantic similarity in hybrid score fusion (0.0 to 1.0).\n",
    "    \n",
    "    Alpha Parameter Behavior:\n",
    "    - alpha = 0.0: Pure text-based ranking (BM25, keyword matching)\n",
    "    - alpha = 0.5: Equal weight for semantic and text relevance\n",
    "    - alpha = 0.75: Semantic similarity prioritized (current setting)\n",
    "    - alpha = 1.0: Pure semantic ranking (cosine similarity only)\n",
    "    \n",
    "    Use Case Recommendations:\n",
    "    - Technical queries: 0.7-0.9 (semantic understanding important)\n",
    "    - Factual queries: 0.5-0.7 (balanced approach)\n",
    "    - Keyword searches: 0.3-0.5 (text matching more important)\n",
    "    - Conversational queries: 0.6-0.8 (semantic context matters)\n",
    "    \n",
    "    Tuning Strategy:\n",
    "    - Start with 0.75 for general use\n",
    "    - Increase if semantic results seem irrelevant\n",
    "    - Decrease if text matching is too weak\n",
    "    \"\"\"\n",
    "    \n",
    "    text_boost: float = 0.20\n",
    "    \"\"\"\n",
    "    Additional score boost for results that match both semantic and text criteria.\n",
    "    \n",
    "    Text Boost Mechanism:\n",
    "    - Applied additively to fused scores\n",
    "    - Encourages results that satisfy both search strategies\n",
    "    - Helps surface highly relevant content that matches multiple criteria\n",
    "    \n",
    "    Boost Value Guidelines:\n",
    "    - Low boost (0.1-0.2): Subtle preference for hybrid matches\n",
    "    - Medium boost (0.2-0.4): Strong preference for hybrid matches\n",
    "    - High boost (0.5+): Heavy preference, may dominate ranking\n",
    "    \n",
    "    Optimal Settings:\n",
    "    - General use: 0.15-0.25\n",
    "    - Technical content: 0.20-0.30\n",
    "    - Factual queries: 0.10-0.20\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # MMR (Maximal Marginal Relevance) Configuration\n",
    "    # =========================\n",
    "    use_mmr: bool = True\n",
    "    \"\"\"\n",
    "    Whether to use MMR for result diversification and redundancy reduction.\n",
    "    \n",
    "    MMR Benefits:\n",
    "    - Reduces redundant results with similar content\n",
    "    - Improves coverage of different aspects of the query\n",
    "    - Better user experience with diverse information\n",
    "    \n",
    "    MMR Trade-offs:\n",
    "    - Slightly slower than simple top-K selection\n",
    "    - May reduce absolute relevance scores\n",
    "    - Better for exploratory queries, worse for specific fact retrieval\n",
    "    \n",
    "    Alternatives:\n",
    "    - False: Simple top-K selection (faster, may have redundancy)\n",
    "    - True: MMR diversification (slower, better diversity)\n",
    "    \"\"\"\n",
    "    \n",
    "    mmr_lambda: float = 0.6\n",
    "    \"\"\"\n",
    "    MMR diversification parameter balancing relevance vs. diversity (0.0 to 1.0).\n",
    "    \n",
    "    Lambda Parameter Behavior:\n",
    "    - lambda = 0.0: Pure diversity (ignore relevance, maximize difference)\n",
    "    - lambda = 0.5: Balanced relevance and diversity\n",
    "    - lambda = 0.6: Slight preference for relevance (current setting)\n",
    "    - lambda = 1.0: Pure relevance (ignore diversity, top-K selection)\n",
    "    \n",
    "    Use Case Recommendations:\n",
    "    - Research queries: 0.4-0.6 (diverse perspectives important)\n",
    "    - Factual queries: 0.7-0.9 (relevance more important)\n",
    "    - Exploratory queries: 0.3-0.5 (diversity valuable)\n",
    "    - Specific searches: 0.8-1.0 (precision over diversity)\n",
    "    \n",
    "    Tuning Guidelines:\n",
    "    - Start with 0.6 for general use\n",
    "    - Decrease if results seem too similar\n",
    "    - Increase if results seem too diverse\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # LLM Configuration (Optional)\n",
    "    # =========================\n",
    "    lm_base_env: str = \"OPENAI_BASE_URL\"\n",
    "    \"\"\"\n",
    "    Environment variable name for LLM service base URL.\n",
    "    \n",
    "    Supported Services:\n",
    "    - OpenAI: https://api.openai.com/v1\n",
    "    - LM Studio: http://localhost:1234/v1\n",
    "    - Ollama: http://localhost:11434/v1\n",
    "    - Custom API: Your endpoint URL\n",
    "    \n",
    "    Configuration Examples:\n",
    "    - OpenAI: OPENAI_BASE_URL=https://api.openai.com/v1\n",
    "    - LM Studio: OPENAI_BASE_URL=http://localhost:1234/v1\n",
    "    - Azure OpenAI: OPENAI_BASE_URL=https://your-resource.openai.azure.com\n",
    "    \"\"\"\n",
    "    \n",
    "    lm_key_env: str = \"OPENAI_API_KEY\"\n",
    "    \"\"\"\n",
    "    Environment variable name for LLM service API key.\n",
    "    \n",
    "    Security Notes:\n",
    "    - Never hardcode API keys in source code\n",
    "    - Use environment variables or secure secret management\n",
    "    - Rotate keys regularly for production systems\n",
    "    \n",
    "    Configuration Examples:\n",
    "    - OpenAI: OPENAI_API_KEY=sk-...\n",
    "    - LM Studio: OPENAI_API_KEY=lm-studio (can be any value)\n",
    "    - Azure: OPENAI_API_KEY=your-azure-key\n",
    "    \"\"\"\n",
    "    \n",
    "    lm_model_env: str = \"LMSTUDIO_MODEL\"\n",
    "    \"\"\"\n",
    "    Environment variable name for the specific LLM model to use.\n",
    "    \n",
    "    Model Selection:\n",
    "    - OpenAI: gpt-3.5-turbo, gpt-4, gpt-4-turbo\n",
    "    - LM Studio: Any model name you've loaded\n",
    "    - Ollama: llama2, codellama, mistral, etc.\n",
    "    - Custom: Your model identifier\n",
    "    \n",
    "    Configuration Examples:\n",
    "    - OpenAI: LMSTUDIO_MODEL=gpt-3.5-turbo\n",
    "    - LM Studio: LMSTUDIO_MODEL=llama-2-7b-chat\n",
    "    - Ollama: LMSTUDIO_MODEL=llama2:7b\n",
    "    \"\"\"\n",
    "\n",
    "SETTINGS = Settings()\n",
    "\n",
    "# =========================\n",
    "# Componenti di base\n",
    "# =========================\n",
    "\n",
    "def get_embeddings(settings: Settings) -> HuggingFaceEmbeddings:\n",
    "    \"\"\"\n",
    "    Initialize and return a HuggingFace embeddings model instance.\n",
    "    \n",
    "    This function creates a sentence transformer model that converts text into\n",
    "    high-dimensional vector representations for semantic similarity search.\n",
    "    \n",
    "    Args:\n",
    "        settings: Configuration object containing the model name and parameters\n",
    "        \n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings: Configured embedding model instance\n",
    "        \n",
    "    Model Loading Behavior:\n",
    "    - First run: Downloads model from HuggingFace Hub (requires internet)\n",
    "    - Subsequent runs: Loads from local cache (~/.cache/huggingface/)\n",
    "    - Model size: 100MB-2GB depending on the selected model\n",
    "        \n",
    "    Performance Notes:\n",
    "    - GPU acceleration: Automatically uses CUDA if available\n",
    "    - CPU fallback: Falls back to CPU if GPU unavailable\n",
    "    - Memory usage: Model loaded into RAM/VRAM during inference\n",
    "        \n",
    "    Error Handling:\n",
    "    - Network issues: Will fail if model not cached and no internet\n",
    "    - Memory issues: Large models may cause OOM on low-memory systems\n",
    "    - Model not found: Invalid model names will cause runtime errors\n",
    "    \"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=settings.hf_model_name)\n",
    "\n",
    "def get_llm(settings: Settings):\n",
    "    \"\"\"\n",
    "    Initialize and test an LLM instance for text generation if properly configured.\n",
    "    \n",
    "    This function attempts to create an LLM connection using environment variables\n",
    "    and performs a connectivity test to ensure the service is working before\n",
    "    returning the instance. If any step fails, it gracefully falls back to None.\n",
    "    \n",
    "    Args:\n",
    "        settings: Configuration object containing LLM environment variable names\n",
    "        \n",
    "    Returns:\n",
    "        ChatModel or None: Configured LLM instance if successful, None otherwise\n",
    "        \n",
    "    Configuration Requirements:\n",
    "    - OPENAI_BASE_URL: Base URL for the LLM service\n",
    "    - OPENAI_API_KEY: Authentication key for the service\n",
    "    - LMSTUDIO_MODEL: Specific model identifier to use\n",
    "        \n",
    "    Supported LLM Services:\n",
    "    - OpenAI API: Production-grade, reliable, paid service\n",
    "    - LM Studio: Local inference, free, requires model download\n",
    "    - Ollama: Local inference, free, easy setup\n",
    "    - Azure OpenAI: Enterprise-grade, reliable, paid service\n",
    "    - Custom APIs: Any OpenAI-compatible endpoint\n",
    "        \n",
    "    Connection Testing:\n",
    "    - Performs a simple \"test\" query to verify connectivity\n",
    "    - Tests both network connectivity and model availability\n",
    "    - Helps identify configuration issues early\n",
    "        \n",
    "    Error Handling Strategy:\n",
    "    - Missing env vars: Graceful fallback with informative message\n",
    "    - Network issues: Catches connection errors and continues\n",
    "    - Authentication errors: Handles invalid API keys gracefully\n",
    "    - Model errors: Catches model-specific issues\n",
    "        \n",
    "    Fallback Behavior:\n",
    "    - Returns None if any step fails\n",
    "    - Script continues without LLM generation\n",
    "    - Retrieved content is displayed instead of generated answers\n",
    "        \n",
    "    Security Considerations:\n",
    "    - API keys are read from environment variables only\n",
    "    - No hardcoded credentials in source code\n",
    "    - Test query is minimal and doesn't expose sensitive data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base = os.getenv(settings.lm_base_env)\n",
    "        key = os.getenv(settings.lm_key_env)\n",
    "        model_name = os.getenv(settings.lm_model_env)\n",
    "        \n",
    "        if not (base and key and model_name):\n",
    "            print(\"LLM not configured - skipping generation step\")\n",
    "            return None\n",
    "            \n",
    "        # Test the LLM connection before returning\n",
    "        llm = init_chat_model(model_name, model_provider=\"openai\")\n",
    "        # Simple test to verify the LLM works\n",
    "        test_response = llm.invoke(\"test\")\n",
    "        if test_response:\n",
    "            print(\"LLM configured successfully\")\n",
    "            return llm\n",
    "        else:\n",
    "            print(\"LLM test failed - skipping generation step\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LLM configuration error: {e}\")\n",
    "        print(\"Continuing without LLM - will show retrieved content only\")\n",
    "        return None\n",
    "\n",
    "def simulate_corpus() -> List[Document]:\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"LangChain is a framework for building applications with Large Language Models. \"\n",
    "                \"It provides chains, agents, prompt templates, memory, and many integrations.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc1\", \"source\": \"intro-langchain.md\", \"title\": \"Intro LangChain\", \"lang\": \"en\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"FAISS is a library for efficient similarity search of dense vectors. \"\n",
    "                \"It supports both exact and approximate nearest neighbor search at scale.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc2\", \"source\": \"faiss-overview.md\", \"title\": \"FAISS Overview\", \"lang\": \"en\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Sentence-transformers like all-MiniLM-L6-v2 produce 384-dimensional sentence embeddings \"\n",
    "                \"for semantic search, clustering, and retrieval-augmented generation.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc3\", \"source\": \"embeddings-minilm.md\", \"title\": \"MiniLM Embeddings\", \"lang\": \"en\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"A typical RAG pipeline includes indexing (load, split, embed, store), retrieval, and generation. \"\n",
    "                \"Retrieval selects the most relevant chunks, then the LLM answers grounded in those chunks.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc4\", \"source\": \"rag-pipeline.md\", \"title\": \"RAG Pipeline\", \"lang\": \"en\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Maximal Marginal Relevance (MMR) trades off relevance and diversity to reduce redundancy \"\n",
    "                \"and improve coverage of distinct aspects in retrieved chunks.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc5\", \"source\": \"retrieval-mmr.md\", \"title\": \"MMR Retrieval\", \"lang\": \"en\"}\n",
    "        ),\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "def split_documents(docs: List[Document], settings: Settings) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=settings.chunk_size,\n",
    "        chunk_overlap=settings.chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \": \", \", \", \" \", \"\"],\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# =========================\n",
    "# Qdrant: creazione collection + indici\n",
    "# =========================\n",
    "\n",
    "def get_qdrant_client(settings: Settings) -> QdrantClient:\n",
    "    return QdrantClient(url=settings.qdrant_url)\n",
    "\n",
    "def recreate_collection_for_rag(client: QdrantClient, settings: Settings, vector_size: int):\n",
    "    \"\"\"\n",
    "    Create or recreate a Qdrant collection optimized for RAG (Retrieval-Augmented Generation).\n",
    "    \n",
    "    This function sets up a vector database collection with optimal configuration for\n",
    "    semantic search, including HNSW indexing, payload indexing, and quantization.\n",
    "    \n",
    "    Args:\n",
    "        client: Qdrant client instance for database operations\n",
    "        settings: Configuration object containing collection parameters\n",
    "        vector_size: Dimension of the embedding vectors (e.g., 384 for MiniLM-L6)\n",
    "        \n",
    "    Collection Architecture:\n",
    "    - Vector storage: Dense vectors for semantic similarity search\n",
    "    - Payload storage: Metadata and text content for retrieval\n",
    "    - Indexing: HNSW for approximate nearest neighbor search\n",
    "    - Quantization: Scalar quantization for memory optimization\n",
    "        \n",
    "    Distance Metric Selection:\n",
    "    - Cosine distance: Normalized similarity, good for semantic embeddings\n",
    "    - Alternatives: Euclidean (L2), Manhattan (L1), Dot product\n",
    "    - Cosine preferred for normalized embeddings (sentence-transformers)\n",
    "        \n",
    "    HNSW Index Configuration:\n",
    "    - m=32: Average connections per node (higher = better quality, more memory)\n",
    "    - ef_construct=256: Search depth during construction (higher = better quality, slower build)\n",
    "    - Trade-offs: Higher values improve recall but increase memory and build time\n",
    "        \n",
    "    Optimizer Configuration:\n",
    "    - default_segment_number=2: Parallel processing segments\n",
    "    - Benefits: Faster indexing, better resource utilization\n",
    "    - Considerations: More segments = more memory overhead\n",
    "        \n",
    "    Quantization Strategy:\n",
    "    - Scalar quantization: Reduces vector precision from float32 to int8\n",
    "    - Memory savings: ~4x reduction in vector storage\n",
    "    - Quality impact: Minimal impact on search accuracy\n",
    "    - always_ram=False: Vectors stored on disk, loaded to RAM as needed\n",
    "        \n",
    "    Payload Indexing Strategy:\n",
    "    - Text index: Full-text search capabilities (BM25 scoring)\n",
    "    - Keyword indices: Fast exact matching and filtering\n",
    "    - Performance: Significantly faster than unindexed field searches\n",
    "        \n",
    "    Collection Lifecycle:\n",
    "    - recreate_collection: Drops existing collection and creates new one\n",
    "    - Use case: Development/testing, major schema changes\n",
    "    - Production: Consider using create_collection + update_collection_info\n",
    "        \n",
    "    Performance Considerations:\n",
    "    - Build time: HNSW construction scales with collection size\n",
    "    - Memory usage: Vectors loaded to RAM during search\n",
    "    - Storage: Quantized vectors + payload data\n",
    "    - Query latency: HNSW provides sub-millisecond search times\n",
    "        \n",
    "    Scaling Guidelines:\n",
    "    - Small collections (<100K vectors): Current settings optimal\n",
    "    - Medium collections (100K-1M vectors): Increase m to 48-64\n",
    "    - Large collections (1M+ vectors): Consider multiple collections or sharding\n",
    "    \"\"\"\n",
    "    client.recreate_collection(\n",
    "        collection_name=settings.collection,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        hnsw_config=HnswConfigDiff(\n",
    "            m=32,             # grado medio del grafo HNSW (maggiore = più memoria/qualità)\n",
    "            ef_construct=256  # ampiezza lista candidati in fase costruzione (qualità/tempo build)\n",
    "        ),\n",
    "        optimizers_config=OptimizersConfigDiff(\n",
    "            default_segment_number=2  # parallelismo/segmentazione iniziale\n",
    "        ),\n",
    "        quantization_config=ScalarQuantization(\n",
    "            scalar=ScalarQuantizationConfig(type=\"int8\", always_ram=False)  # on-disk quantization dei vettori\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Indice full-text sul campo 'text' per filtri MatchText\n",
    "    client.create_payload_index(\n",
    "        collection_name=settings.collection,\n",
    "        field_name=\"text\",\n",
    "        field_schema=PayloadSchemaType.TEXT\n",
    "    )\n",
    "\n",
    "    # Indici keyword per filtri esatti / velocità nei filtri\n",
    "    for key in [\"doc_id\", \"source\", \"title\", \"lang\"]:\n",
    "        client.create_payload_index(\n",
    "            collection_name=settings.collection,\n",
    "            field_name=key,\n",
    "            field_schema=PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "\n",
    "# =========================\n",
    "# Ingest: chunk -> embed -> upsert\n",
    "# =========================\n",
    "\n",
    "def build_points(chunks: List[Document], embeds: List[List[float]]) -> List[PointStruct]:\n",
    "    pts: List[PointStruct] = []\n",
    "    for i, (doc, vec) in enumerate(zip(chunks, embeds), start=1):\n",
    "        payload = {\n",
    "            \"doc_id\": doc.metadata.get(\"id\"),\n",
    "            \"source\": doc.metadata.get(\"source\"),\n",
    "            \"title\": doc.metadata.get(\"title\"),\n",
    "            \"lang\": doc.metadata.get(\"lang\", \"en\"),\n",
    "            \"text\": doc.page_content,\n",
    "            \"chunk_id\": i - 1\n",
    "        }\n",
    "        pts.append(PointStruct(id=i, vector=vec, payload=payload))\n",
    "    return pts\n",
    "\n",
    "def upsert_chunks(client: QdrantClient, settings: Settings, chunks: List[Document], embeddings: HuggingFaceEmbeddings):\n",
    "    vecs = embeddings.embed_documents([c.page_content for c in chunks])\n",
    "    points = build_points(chunks, vecs)\n",
    "    client.upsert(collection_name=settings.collection, points=points, wait=True)\n",
    "\n",
    "# =========================\n",
    "# Ricerca: semantica / testuale / ibrida\n",
    "# =========================\n",
    "\n",
    "def qdrant_semantic_search(\n",
    "    client: QdrantClient,\n",
    "    settings: Settings,\n",
    "    query: str,\n",
    "    embeddings: HuggingFaceEmbeddings,\n",
    "    limit: int,\n",
    "    with_vectors: bool = False\n",
    "):\n",
    "    qv = embeddings.embed_query(query)\n",
    "    res = client.query_points(\n",
    "        collection_name=settings.collection,\n",
    "        query=qv,\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "        with_vectors=with_vectors,\n",
    "        search_params=SearchParams(\n",
    "            hnsw_ef=256,  # ampiezza lista in fase di ricerca (recall/latency)\n",
    "            exact=False   # True = ricerca esatta (lenta); False = ANN HNSW\n",
    "        ),\n",
    "    )\n",
    "    return res.points\n",
    "\n",
    "def qdrant_text_prefilter_ids(\n",
    "    client: QdrantClient,\n",
    "    settings: Settings,\n",
    "    query: str,\n",
    "    max_hits: int\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Usa l'indice full-text su 'text' per prefiltrare i punti che contengono parole chiave.\n",
    "    Non restituisce uno score BM25: otteniamo un sottoinsieme di id da usare come boost.\n",
    "    \"\"\"\n",
    "    # Scroll con filtro MatchText per ottenere id dei match testuali\n",
    "    # (nota: scroll è paginato; qui prendiamo solo i primi max_hits per semplicità)\n",
    "    matched_ids: List[int] = []\n",
    "    next_page = None\n",
    "    while True:\n",
    "        points, next_page = client.scroll(\n",
    "            collection_name=settings.collection,\n",
    "            scroll_filter=Filter(\n",
    "                must=[FieldCondition(key=\"text\", match=MatchText(text=query))]\n",
    "            ),\n",
    "            limit=min(256, max_hits - len(matched_ids)),\n",
    "            offset=next_page,\n",
    "            with_payload=False,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        matched_ids.extend([p.id for p in points])\n",
    "        if not next_page or len(matched_ids) >= max_hits:\n",
    "            break\n",
    "    return matched_ids\n",
    "\n",
    "def mmr_select(\n",
    "    query_vec: List[float],\n",
    "    candidates_vecs: List[List[float]],\n",
    "    k: int,\n",
    "    lambda_mult: float\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Select diverse results using Maximal Marginal Relevance (MMR) algorithm.\n",
    "    \n",
    "    MMR balances relevance to the query with diversity among selected results,\n",
    "    reducing redundancy and improving information coverage. This is particularly\n",
    "    useful for RAG systems where diverse context provides better generation.\n",
    "    \n",
    "    Args:\n",
    "        query_vec: Query embedding vector for relevance calculation\n",
    "        candidates_vecs: List of candidate document embedding vectors\n",
    "        k: Number of results to select\n",
    "        lambda_mult: MMR parameter balancing relevance vs. diversity (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: Indices of selected candidates in order of selection\n",
    "        \n",
    "    MMR Algorithm Overview:\n",
    "    \n",
    "    The algorithm iteratively selects candidates that maximize the MMR score:\n",
    "    \n",
    "    MMR_score(i) = λ × Relevance(i, query) - (1-λ) × max_similarity(i, selected)\n",
    "    \n",
    "    Where:\n",
    "    - λ (lambda_mult): Weight for relevance vs. diversity\n",
    "    - Relevance(i, query): Cosine similarity between candidate i and query\n",
    "    - max_similarity(i, selected): Maximum similarity between candidate i and already selected items\n",
    "        \n",
    "    Algorithm Steps:\n",
    "    \n",
    "    1. INITIALIZATION:\n",
    "       - Calculate relevance scores for all candidates vs. query\n",
    "       - Select the highest-scoring candidate as the first result\n",
    "       - Initialize selected and remaining candidate sets\n",
    "        \n",
    "    2. ITERATIVE SELECTION:\n",
    "       - For each remaining position, calculate MMR score for all candidates\n",
    "       - MMR score balances query relevance with diversity from selected items\n",
    "       - Select candidate with highest MMR score\n",
    "       - Update selected and remaining sets\n",
    "        \n",
    "    3. TERMINATION:\n",
    "       - Continue until k candidates selected or no more candidates available\n",
    "       - Return indices in selection order\n",
    "        \n",
    "    Mathematical Foundation:\n",
    "    \n",
    "    Cosine Similarity:\n",
    "    - cos(a,b) = (a·b) / (||a|| × ||b||)\n",
    "    - Range: [-1, 1] where 1 = identical, 0 = orthogonal, -1 = opposite\n",
    "    - Normalized vectors typically have values in [0, 1] range\n",
    "        \n",
    "    MMR Score Calculation:\n",
    "    - Relevance term: λ × cos(query, candidate)\n",
    "    - Diversity term: (1-λ) × max(cos(candidate, selected_i))\n",
    "    - Higher relevance increases score, higher similarity to selected decreases score\n",
    "        \n",
    "    Lambda Parameter Behavior:\n",
    "    \n",
    "    λ = 0.0 (Pure Diversity):\n",
    "    - Only diversity matters, relevance ignored\n",
    "    - Results may be irrelevant to query\n",
    "    - Useful for exploratory search\n",
    "        \n",
    "    λ = 0.5 (Balanced):\n",
    "    - Equal weight for relevance and diversity\n",
    "    - Good compromise for general use\n",
    "    - Moderate redundancy reduction\n",
    "        \n",
    "    λ = 0.6 (Current Setting):\n",
    "    - Slight preference for relevance\n",
    "    - Good diversity while maintaining relevance\n",
    "    - Recommended for most RAG applications\n",
    "        \n",
    "    λ = 1.0 (Pure Relevance):\n",
    "    - Only relevance matters, diversity ignored\n",
    "    - Equivalent to simple top-K selection\n",
    "    - May have redundant results\n",
    "        \n",
    "    Performance Characteristics:\n",
    "    \n",
    "    Time Complexity:\n",
    "    - O(k × n) where k = results to select, n = total candidates\n",
    "    - Each iteration processes all remaining candidates\n",
    "    - Quadratic complexity in worst case (k ≈ n)\n",
    "        \n",
    "    Space Complexity:\n",
    "    - O(n) for storing vectors and similarity scores\n",
    "    - O(k) for selected indices\n",
    "    - O(n) for remaining candidate set\n",
    "        \n",
    "    Memory Usage:\n",
    "    - Vector storage: All candidate vectors loaded in memory\n",
    "    - Similarity cache: Relevance scores computed once\n",
    "    - Selection state: Small overhead for tracking\n",
    "        \n",
    "    Quality Metrics:\n",
    "    \n",
    "    Relevance Preservation:\n",
    "    - Higher lambda values preserve more relevance\n",
    "    - Lower lambda values may sacrifice relevance for diversity\n",
    "    - Optimal balance depends on use case\n",
    "        \n",
    "    Diversity Improvement:\n",
    "    - MMR significantly reduces redundancy compared to top-K\n",
    "    - Diversity increases as lambda decreases\n",
    "    - Measurable improvement in information coverage\n",
    "        \n",
    "    User Experience:\n",
    "    - Less repetitive results\n",
    "    - Better coverage of different aspects\n",
    "    - More informative context for LLM generation\n",
    "        \n",
    "    Use Case Recommendations:\n",
    "    \n",
    "    Research & Exploration:\n",
    "    - λ = 0.3-0.5: Maximize diversity for comprehensive understanding\n",
    "    - Higher k values: More diverse perspectives\n",
    "        \n",
    "    Factual Queries:\n",
    "    - λ = 0.7-0.9: Prioritize relevance for accurate information\n",
    "    - Lower k values: Focus on most relevant results\n",
    "        \n",
    "    Technical Documentation:\n",
    "    - λ = 0.5-0.7: Balance relevance with diverse technical perspectives\n",
    "    - Moderate k values: Comprehensive technical coverage\n",
    "        \n",
    "    Conversational AI:\n",
    "    - λ = 0.6-0.8: Good relevance with some diversity\n",
    "    - Higher k values: Rich context for generation\n",
    "        \n",
    "    Tuning Guidelines:\n",
    "    \n",
    "    For Maximum Diversity:\n",
    "    - Decrease lambda to 0.3-0.5\n",
    "    - Increase k to 8-12 results\n",
    "    - Monitor relevance quality\n",
    "        \n",
    "    For Maximum Relevance:\n",
    "    - Increase lambda to 0.8-1.0\n",
    "    - Decrease k to 3-6 results\n",
    "    - Accept some redundancy\n",
    "        \n",
    "    For Balanced Results:\n",
    "    - Use lambda = 0.6-0.7 (current setting)\n",
    "    - Moderate k values (6-8)\n",
    "    - Good compromise for most applications\n",
    "        \n",
    "    Implementation Notes:\n",
    "    \n",
    "    Numerical Stability:\n",
    "    - Small epsilon (1e-12) added to prevent division by zero\n",
    "    - Cosine similarity handles normalized vectors robustly\n",
    "    - Float precision sufficient for similarity calculations\n",
    "        \n",
    "    Edge Cases:\n",
    "    - Empty candidate list: Returns empty result\n",
    "    - k > candidates: Returns all candidates\n",
    "    - Single candidate: Returns that candidate regardless of lambda\n",
    "        \n",
    "    Optimization Opportunities:\n",
    "    - Vector similarity could be pre-computed and cached\n",
    "    - Parallel processing for large candidate sets\n",
    "    - Early termination for very low diversity scores\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    V = np.array(candidates_vecs, dtype=float)\n",
    "    q = np.array(query_vec, dtype=float)\n",
    "\n",
    "    def cos(a, b):\n",
    "        na = (a @ a) ** 0.5 + 1e-12\n",
    "        nb = (b @ b) ** 0.5 + 1e-12\n",
    "        return float((a @ b) / (na * nb))\n",
    "\n",
    "    sims = [cos(v, q) for v in V]\n",
    "    selected: List[int] = []\n",
    "    remaining = set(range(len(V)))\n",
    "\n",
    "    while len(selected) < min(k, len(V)):\n",
    "        if not selected:\n",
    "            # pick the highest similarity first\n",
    "            best = max(remaining, key=lambda i: sims[i])\n",
    "            selected.append(best)\n",
    "            remaining.remove(best)\n",
    "            continue\n",
    "        best_idx = None\n",
    "        best_score = -1e9\n",
    "        for i in remaining:\n",
    "            max_div = max([cos(V[i], V[j]) for j in selected]) if selected else 0.0\n",
    "            score = lambda_mult * sims[i] - (1 - lambda_mult) * max_div\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "        selected.append(best_idx)\n",
    "        remaining.remove(best_idx)\n",
    "    return selected\n",
    "\n",
    "def hybrid_search(\n",
    "    client: QdrantClient,\n",
    "    settings: Settings,\n",
    "    query: str,\n",
    "    embeddings: HuggingFaceEmbeddings\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining semantic similarity and text-based matching.\n",
    "    \n",
    "    This function implements a sophisticated retrieval strategy that leverages both\n",
    "    semantic understanding and traditional text search to provide high-quality,\n",
    "    relevant results with minimal redundancy.\n",
    "    \n",
    "    Args:\n",
    "        client: Qdrant client for database operations\n",
    "        settings: Configuration object containing search parameters\n",
    "        query: User's search query string\n",
    "        embeddings: Embedding model for semantic search\n",
    "        \n",
    "    Returns:\n",
    "        List[ScoredPoint]: Ranked list of relevant document chunks\n",
    "        \n",
    "    Hybrid Search Strategy Overview:\n",
    "    \n",
    "    1. SEMANTIC SEARCH (Vector Similarity):\n",
    "       - Converts query to embedding vector\n",
    "       - Performs approximate nearest neighbor search using HNSW index\n",
    "       - Retrieves top_n_semantic candidates based on cosine similarity\n",
    "       - Provides semantic understanding of query intent\n",
    "        \n",
    "    2. TEXT-BASED PREFILTERING:\n",
    "       - Uses full-text search capabilities (BM25 scoring)\n",
    "       - Identifies documents containing query keywords/phrases\n",
    "       - Creates a set of text-relevant document IDs\n",
    "       - Acts as a relevance filter for semantic results\n",
    "        \n",
    "    3. SCORE FUSION & NORMALIZATION:\n",
    "       - Normalizes semantic scores to [0,1] range for fair comparison\n",
    "       - Applies alpha weight to balance semantic vs. text relevance\n",
    "       - Adds text_boost for results matching both criteria\n",
    "       - Creates unified relevance scoring\n",
    "        \n",
    "    4. RESULT DIVERSIFICATION (Optional MMR):\n",
    "       - Applies Maximal Marginal Relevance to reduce redundancy\n",
    "       - Balances relevance with diversity using mmr_lambda parameter\n",
    "       - Selects final_k results from top candidates\n",
    "        \n",
    "    Algorithm Flow:\n",
    "    \n",
    "    Phase 1: Semantic Retrieval\n",
    "    - Query embedding generation\n",
    "    - HNSW-based vector search\n",
    "    - Score normalization for fusion\n",
    "        \n",
    "    Phase 2: Text Matching\n",
    "    - Full-text search with MatchText filter\n",
    "    - ID collection for hybrid scoring\n",
    "    - Performance optimization with pagination\n",
    "        \n",
    "    Phase 3: Score Fusion\n",
    "    - Linear combination of semantic and text scores\n",
    "    - Boost application for hybrid matches\n",
    "    - Ranking by fused scores\n",
    "        \n",
    "    Phase 4: Result Selection\n",
    "    - Top-N selection or MMR diversification\n",
    "    - Final result ordering and return\n",
    "        \n",
    "    Performance Characteristics:\n",
    "    \n",
    "    Time Complexity:\n",
    "    - Semantic search: O(log n) with HNSW index\n",
    "    - Text search: O(m) where m is text matches\n",
    "    - Score fusion: O(k) where k is semantic candidates\n",
    "    - MMR: O(k²) for diversity computation\n",
    "        \n",
    "    Memory Usage:\n",
    "    - Vector storage: Quantized vectors in memory\n",
    "    - Score storage: Temporary arrays for fusion\n",
    "    - Result storage: Final selected points\n",
    "        \n",
    "    Quality Metrics:\n",
    "    \n",
    "    Recall (Completeness):\n",
    "    - Semantic search: High recall for conceptual queries\n",
    "    - Text search: High recall for keyword queries\n",
    "    - Hybrid approach: Combines strengths of both\n",
    "        \n",
    "    Precision (Relevance):\n",
    "    - Score fusion: Balances multiple relevance signals\n",
    "    - Text boost: Rewards multi-criteria matches\n",
    "    - MMR: Reduces redundant results\n",
    "        \n",
    "    Diversity:\n",
    "    - MMR algorithm: Maximizes information coverage\n",
    "    - Lambda parameter: Controls diversity vs. relevance trade-off\n",
    "    - Result variety: Better user experience\n",
    "        \n",
    "    Tuning Guidelines:\n",
    "    \n",
    "    For High Precision:\n",
    "    - Increase alpha (0.8-0.9): Prioritize semantic similarity\n",
    "    - Increase text_boost (0.3-0.5): Reward text matches\n",
    "    - Decrease mmr_lambda (0.7-0.9): Prioritize relevance\n",
    "        \n",
    "    For High Recall:\n",
    "    - Increase top_n_semantic (50-100): More candidates\n",
    "    - Increase top_n_text (150-200): More text matches\n",
    "    - Decrease alpha (0.5-0.7): Balance search strategies\n",
    "        \n",
    "    For High Diversity:\n",
    "    - Enable MMR (use_mmr=True)\n",
    "    - Decrease mmr_lambda (0.3-0.6): Prioritize diversity\n",
    "    - Increase final_k (8-12): More diverse results\n",
    "        \n",
    "    Use Case Optimizations:\n",
    "    \n",
    "    Technical Documentation:\n",
    "    - High alpha (0.8-0.9): Semantic understanding critical\n",
    "    - High text_boost (0.3-0.4): Technical terms important\n",
    "    - MMR enabled: Diverse technical perspectives\n",
    "        \n",
    "    General Knowledge:\n",
    "    - Balanced alpha (0.6-0.8): Both strategies valuable\n",
    "    - Moderate text_boost (0.2-0.3): Balanced approach\n",
    "    - MMR enabled: Comprehensive coverage\n",
    "        \n",
    "    Factual Queries:\n",
    "    - High alpha (0.7-0.9): Semantic context important\n",
    "    - Low text_boost (0.1-0.2): Facts over style\n",
    "    - MMR optional: Precision over diversity\n",
    "    \"\"\"\n",
    "    # (1) semantica\n",
    "    sem = qdrant_semantic_search(\n",
    "        client, settings, query, embeddings,\n",
    "        limit=settings.top_n_semantic, with_vectors=True\n",
    "    )\n",
    "    if not sem:\n",
    "        return []\n",
    "\n",
    "    # (2) full-text prefilter (id)\n",
    "    text_ids = set(qdrant_text_prefilter_ids(client, settings, query, settings.top_n_text))\n",
    "\n",
    "    # Normalizzazione score semantici per fusione\n",
    "    scores = [p.score for p in sem]\n",
    "    smin, smax = min(scores), max(scores)\n",
    "    def norm(x):  # robusto al caso smin==smax\n",
    "        return 1.0 if smax == smin else (x - smin) / (smax - smin)\n",
    "\n",
    "    # (3) fusione con boost testuale\n",
    "    fused: List[Tuple[int, float, Any]] = []  # (idx, fused_score, point)\n",
    "    for idx, p in enumerate(sem):\n",
    "        base = norm(p.score)                    # [0..1]\n",
    "        fuse = settings.alpha * base\n",
    "        if p.id in text_ids:\n",
    "            fuse += settings.text_boost         # boost additivo\n",
    "        fused.append((idx, fuse, p))\n",
    "\n",
    "    # ordina per fused_score desc\n",
    "    fused.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "    # MMR opzionale per diversificare i top-K\n",
    "    if settings.use_mmr:\n",
    "        qv = embeddings.embed_query(query)\n",
    "        # prendiamo i primi N dopo fusione (es. 30) e poi MMR per final_k\n",
    "        N = min(len(fused), max(settings.final_k * 5, settings.final_k))\n",
    "        cut = fused[:N]\n",
    "        vecs = [sem[i].vector for i, _, _ in cut]\n",
    "        mmr_idx = mmr_select(qv, vecs, settings.final_k, settings.mmr_lambda)\n",
    "        picked = [cut[i][2] for i in mmr_idx]\n",
    "        return picked\n",
    "\n",
    "    # altrimenti, prendi i primi final_k dopo fusione\n",
    "    return [p for _, _, p in fused[:settings.final_k]]\n",
    "\n",
    "# =========================\n",
    "# Prompt/Chain per generazione con citazioni\n",
    "# =========================\n",
    "\n",
    "def format_docs_for_prompt(points: Iterable[Any]) -> str:\n",
    "    blocks = []\n",
    "    for p in points:\n",
    "        pay = p.payload or {}\n",
    "        src = pay.get(\"source\", \"unknown\")\n",
    "        blocks.append(f\"[source:{src}] {pay.get('text','')}\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "def build_rag_chain(llm):\n",
    "    system_prompt = (\n",
    "        \"Sei un assistente tecnico. Rispondi in italiano, conciso e accurato. \"\n",
    "        \"Usa ESCLUSIVAMENTE le informazioni presenti nel CONTENUTO. \"\n",
    "        \"Se non è presente, dichiara: 'Non è presente nel contesto fornito.' \"\n",
    "        \"Cita sempre le fonti nel formato [source:FILE].\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\",\n",
    "         \"Domanda:\\n{question}\\n\\n\"\n",
    "         \"CONTENUTO:\\n{context}\\n\\n\"\n",
    "         \"Istruzioni:\\n\"\n",
    "         \"1) Risposta basata solo sul contenuto.\\n\"\n",
    "         \"2) Includi citazioni [source:...].\\n\"\n",
    "         \"3) Niente invenzioni.\")\n",
    "    ])\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": RunnablePassthrough(),  # stringa già formattata\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# =========================\n",
    "# Main end-to-end demo\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the complete RAG pipeline.\n",
    "    \n",
    "    This function orchestrates the entire RAG workflow from document ingestion\n",
    "    to intelligent question answering, showcasing the system's capabilities\n",
    "    and providing a template for production deployment.\n",
    "    \n",
    "    Pipeline Overview:\n",
    "    \n",
    "    1. SYSTEM INITIALIZATION:\n",
    "       - Load configuration settings\n",
    "       - Initialize embedding model\n",
    "       - Configure LLM (optional)\n",
    "       - Establish database connection\n",
    "        \n",
    "    2. DOCUMENT PROCESSING:\n",
    "       - Load or simulate document corpus\n",
    "       - Split documents into manageable chunks\n",
    "       - Generate vector embeddings for each chunk\n",
    "        \n",
    "    3. VECTOR DATABASE SETUP:\n",
    "       - Create/configure Qdrant collection\n",
    "       - Set up HNSW indexing and payload indices\n",
    "       - Optimize for semantic search performance\n",
    "        \n",
    "    4. DATA INGESTION:\n",
    "       - Store document chunks with metadata\n",
    "       - Index vectors for fast retrieval\n",
    "       - Ensure data consistency and availability\n",
    "        \n",
    "    5. INTELLIGENT RETRIEVAL:\n",
    "       - Process user queries through hybrid search\n",
    "       - Combine semantic and text-based matching\n",
    "       - Apply MMR for result diversification\n",
    "        \n",
    "    6. CONTENT GENERATION:\n",
    "       - Use LLM for intelligent answer generation\n",
    "       - Fall back to content display if LLM unavailable\n",
    "       - Provide source citations and context\n",
    "        \n",
    "    Performance Characteristics:\n",
    "    \n",
    "    Initialization Time:\n",
    "    - Embedding model: 2-10 seconds (depends on model size)\n",
    "    - LLM connection: 0.1-5 seconds (depends on service)\n",
    "    - Database setup: 1-5 seconds (depends on collection size)\n",
    "        \n",
    "    Processing Time:\n",
    "    - Document chunking: Linear with document count\n",
    "    - Vector generation: Linear with chunk count\n",
    "    - Database indexing: O(n log n) with HNSW construction\n",
    "        \n",
    "    Query Time:\n",
    "    - Semantic search: Sub-millisecond with HNSW\n",
    "    - Text search: Millisecond range with payload indices\n",
    "    - Result fusion: Linear with candidate count\n",
    "    - MMR diversification: Quadratic with candidate count\n",
    "        \n",
    "    Memory Usage:\n",
    "    - Embedding model: 100MB-2GB (depends on model)\n",
    "    - Vector storage: 4 bytes × dimensions × chunks (quantized)\n",
    "    - Payload storage: Variable based on metadata size\n",
    "    - LLM context: Depends on model and input size\n",
    "        \n",
    "    Scalability Considerations:\n",
    "    \n",
    "    Document Volume:\n",
    "    - Small (<1K docs): Current settings optimal\n",
    "    - Medium (1K-100K docs): Consider batch processing\n",
    "    - Large (100K+ docs): Implement streaming ingestion\n",
    "        \n",
    "    Vector Dimensions:\n",
    "    - 384 dimensions: Fast, memory-efficient, good quality\n",
    "    - 768 dimensions: Higher quality, more memory, slower\n",
    "    - 1024+ dimensions: Maximum quality, significant overhead\n",
    "        \n",
    "    Collection Management:\n",
    "    - Single collection: Simple, good for small-medium datasets\n",
    "    - Multiple collections: Better for large, diverse datasets\n",
    "    - Sharding: Consider for very large datasets (>1M vectors)\n",
    "        \n",
    "    Error Handling Strategy:\n",
    "    \n",
    "    Graceful Degradation:\n",
    "    - LLM failures: Fall back to content display\n",
    "    - Database errors: Informative error messages\n",
    "    - Network issues: Retry logic for transient failures\n",
    "        \n",
    "    Resource Management:\n",
    "    - Memory monitoring: Prevent OOM conditions\n",
    "    - Connection pooling: Efficient database usage\n",
    "    - Cleanup: Proper resource deallocation\n",
    "        \n",
    "    Monitoring & Logging:\n",
    "    - Performance metrics: Track response times\n",
    "    - Error rates: Monitor system health\n",
    "    - Usage patterns: Understand user behavior\n",
    "        \n",
    "    Production Deployment Considerations:\n",
    "    \n",
    "    Environment Configuration:\n",
    "    - Use environment variables for sensitive data\n",
    "    - Separate configs for dev/staging/production\n",
    "    - Implement proper logging and monitoring\n",
    "        \n",
    "    Security:\n",
    "    - API key management: Secure storage and rotation\n",
    "    - Network security: HTTPS, firewall rules\n",
    "    - Access control: User authentication and authorization\n",
    "        \n",
    "    Performance Optimization:\n",
    "    - Caching: Redis for frequently accessed data\n",
    "    - Load balancing: Distribute requests across instances\n",
    "    - CDN: Static content delivery optimization\n",
    "        \n",
    "    Maintenance:\n",
    "    - Regular backups: Database and configuration\n",
    "    - Model updates: Periodic embedding model refresh\n",
    "    - Performance tuning: Monitor and adjust parameters\n",
    "    \"\"\"\n",
    "    s = SETTINGS\n",
    "    embeddings = get_embeddings(s)\n",
    "    llm = get_llm(s)  # opzionale\n",
    "\n",
    "    # 1) Client Qdrant\n",
    "    client = get_qdrant_client(s)\n",
    "\n",
    "    # 2) Dati -> chunk\n",
    "    docs = simulate_corpus()\n",
    "    chunks = split_documents(docs, s)\n",
    "\n",
    "    # 3) Crea (o ricrea) collection\n",
    "    vector_size = embeddings._client.get_sentence_embedding_dimension()\n",
    "    recreate_collection_for_rag(client, s, vector_size)\n",
    "\n",
    "    # 4) Upsert chunks\n",
    "    upsert_chunks(client, s, chunks, embeddings)\n",
    "\n",
    "    # 5) Query ibrida\n",
    "    questions = [\n",
    "        \"Cos'è una pipeline RAG e quali sono le sue fasi?\",\n",
    "        \"A cosa serve FAISS e che caratteristiche offre?\",\n",
    "        \"Che cos'è MMR e perché riduce la ridondanza?\",\n",
    "        \"Qual è la dimensione degli embedding di all-MiniLM-L6-v2?\",\n",
    "    ]\n",
    "\n",
    "    for q in questions:\n",
    "        hits = hybrid_search(client, s, q, embeddings)\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Q:\", q)\n",
    "        if not hits:\n",
    "            print(\"Nessun risultato.\")\n",
    "            continue\n",
    "\n",
    "        # Mostra id/score di debug\n",
    "        for p in hits:\n",
    "            print(f\"- id={p.id} score={p.score:.4f} src={p.payload.get('source')}\")\n",
    "\n",
    "        # Se LLM configurato: genera\n",
    "        if llm:\n",
    "            try:\n",
    "                ctx = format_docs_for_prompt(hits)\n",
    "                chain = build_rag_chain(llm)\n",
    "                answer = chain.invoke({\"question\": q, \"context\": ctx})\n",
    "                print(\"\\n\", answer, \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nLLM generation failed: {e}\")\n",
    "                print(\"Falling back to content display...\")\n",
    "                print(\"\\nContenuto recuperato:\\n\")\n",
    "                print(format_docs_for_prompt(hits))\n",
    "                print()\n",
    "        else:\n",
    "            # Fallback: stampa i chunk per ispezione\n",
    "            print(\"\\nContenuto recuperato:\\n\")\n",
    "            print(format_docs_for_prompt(hits))\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Come funziona la **Hybrid Search** (scelta progettuale semplice & robusta)\n",
    "\n",
    "Qdrant fornisce:\n",
    "\n",
    "* **Similarità vettoriale** (ANN HNSW) sul campo `vector` con metrica (qui **COSINE**).\n",
    "* **Full-text indexing** sul payload testuale (campo `text`) con **MatchText** (filtra i punti che contengono certe parole; non calcola un punteggio BM25 pubblico).\n",
    "\n",
    "Per ottenere un comportamento “ibrido” (semantico **+** corrispondenza testuale):\n",
    "\n",
    "1. **Ricerca semantica**: ottieni i **top\\_N** punti più simili (score alto → più similitudine).\n",
    "2. **Prefiltro testuale**: ottieni l’insieme di `id` che **matchano** il testo della query tramite `MatchText`.\n",
    "3. **Fusione**: normalizza gli score semantici in `[0,1]` e applica un **boost** additivo a chi matcha anche il testo. Ordina per score fuso.\n",
    "4. **MMR (facoltativo)**: esegui un re-ranking locale per ridurre la ridondanza e coprire aspetti diversi (parametro `mmr_lambda`).\n",
    "\n",
    "> Questo approccio è **semplice** e sfrutta **solo Qdrant** (niente BM25 esterno). In alternativa, puoi implementare un vero **BM25** locale o usare una libreria di re-ranking (cross-encoder) per una ibridazione più sofisticata (alpha·cosine + (1-alpha)·bm25\\_norm).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Spiegazione di **tutti i parametri Qdrant** usati\n",
    "\n",
    "### Collection & vettori\n",
    "\n",
    "* `VectorParams(size, distance)`: definisce dimensione e metrica del vettore.\n",
    "\n",
    "  * **size**: deve corrispondere alla dimensione dell’embedding (es. 384 per `all-MiniLM-L6-v2`).\n",
    "  * **distance**: metrica tra vettori. **COSINE** è una scelta tipica per embedding semantici; alternative: `DOT`, `EUCLID`.\n",
    "\n",
    "### Indice HNSW (Approximate Nearest Neighbors)\n",
    "\n",
    "* `HnswConfigDiff(m, ef_construct)`:\n",
    "\n",
    "  * **m**: grado medio del grafo HNSW. Aumentarlo → più memoria e tempi di build, ma **migliore recall**.\n",
    "  * **ef\\_construct**: ampiezza lista candidati in costruzione. Valori più alti → build più lenta, **qualità** migliore dell’indice.\n",
    "\n",
    "### Parametri di ricerca\n",
    "\n",
    "* `SearchParams(hnsw_ef, exact)`:\n",
    "\n",
    "  * **hnsw\\_ef**: ampiezza lista durante la **ricerca**. Aumentarlo aumenta **recall** ma anche **latenza**. Tipico 128–512.\n",
    "  * **exact**: se `True`, fa una ricerca **esatta** (non ANN), molto più lenta su dataset grandi; di solito `False`.\n",
    "\n",
    "### Ottimizzatori / segmenti\n",
    "\n",
    "* `OptimizersConfigDiff(default_segment_number)`:\n",
    "\n",
    "  * Numero di segmenti iniziali per la collection. Più segmenti possono aiutare in ingest parallelo e scalabilità, ma su dataset piccoli 1–2 è ok.\n",
    "\n",
    "### Quantizzazione\n",
    "\n",
    "* `ScalarQuantization(ScalarQuantizationConfig(always_ram=False))`:\n",
    "\n",
    "  * Abilita quantizzazione scalare (riduce spazio e RAM).\n",
    "  * **always\\_ram=False**: consente l’uso on-disk; riduce RAM a scapito di un po’ di latenza.\n",
    "\n",
    "### Indici di payload\n",
    "\n",
    "* `create_payload_index(field_name=\"text\", field_schema=PayloadSchemaType.TEXT)`:\n",
    "\n",
    "  * Crea un indice **full-text** sul campo `text`, necessario per `MatchText`.\n",
    "* `create_payload_index(..., field_schema=PayloadSchemaType.KEYWORD)`:\n",
    "\n",
    "  * Crea indici per **match esatto**/filtri veloci su campi come `doc_id`, `source`, `title`, `lang`.\n",
    "\n",
    "### Upsert\n",
    "\n",
    "* `client.upsert(points=[PointStruct(... )], wait=True)`:\n",
    "\n",
    "  * Inserisce/aggiorna punti. `wait=True` attende la persistenza.\n",
    "\n",
    "### Query\n",
    "\n",
    "* `client.query_points(query=vector, limit=k, with_payload=True, with_vectors=False, search_params=...)`:\n",
    "\n",
    "  * Ricerca vettoriale **ANN** (o esatta se `exact=True`).\n",
    "  * **with\\_payload**: restituisce il payload (testo, metadati) per costruire prompt/risposta.\n",
    "  * **with\\_vectors**: utile quando vuoi applicare MMR lato client.\n",
    "\n",
    "### Filtri\n",
    "\n",
    "* `Filter(must=[FieldCondition(key=\"text\", match=MatchText(text=\"...\"))])`:\n",
    "\n",
    "  * Full-text **prefilter** sul campo `text`. Ritorna solo punti che contengono i termini (non fornisce punteggio BM25 pubblico).\n",
    "* `FieldCondition(key=\"doc_id\", match=MatchValue(value=\"...\"))`:\n",
    "\n",
    "  * Filtro **keyword** per match esatto.\n",
    "\n",
    "### Scroll\n",
    "\n",
    "* `client.scroll(scroll_filter=..., limit=..., offset=...)`:\n",
    "\n",
    "  * Scansione paginata di punti che soddisfano un filtro. Qui usata per ottenere un **insieme di id** che matchano il testo.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Parametri chiave della **Hybrid Search** (nel codice)\n",
    "\n",
    "* `top_n_semantic`: quanti candidati prendere dalla ricerca semantica iniziale (es. 30).\n",
    "* `top_n_text`: massimo punti da considerare dal pre-filtro testuale (es. 100).\n",
    "* `alpha`: peso della componente **semantica** (0..1). 0.75 = priorità alla similarità vettoriale.\n",
    "* `text_boost`: bonus additivo alle entry che matchano anche il **testo** (innalza la posizione in classifica).\n",
    "* `final_k`: quanti risultati finali vuoi dopo fusione e re-ranking.\n",
    "* `use_mmr`, `mmr_lambda`: se attivo, applica MMR ai top-N per diversificare (0→massima diversità, 1→massima rilevanza; tipico 0.5–0.7).\n",
    "\n",
    "> Consiglio pratico: inizia con `alpha=0.7–0.85` e `text_boost=0.15–0.3`; poi calibra su un set di domande/risposte atteso.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Come eseguire\n",
    "\n",
    "```bash\n",
    "pip install qdrant-client sentence-transformers langchain-core langchain-huggingface python-dotenv\n",
    "python rag_qdrant_hybrid.py\n",
    "```\n",
    "\n",
    "Se vuoi anche la **generazione** con un LLM:\n",
    "\n",
    "1. Imposta variabili:\n",
    "\n",
    "   * `OPENAI_BASE_URL` (es. `http://localhost:1234/v1` per LM Studio)\n",
    "   * `OPENAI_API_KEY` (qualunque stringa per LM Studio; reale per OpenAI)\n",
    "   * `LMSTUDIO_MODEL` (o il nome modello per il tuo provider OpenAI-compatible)\n",
    "2. Rilancia lo script: vedrai le risposte con **citazioni \\[source:...]**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Estensioni utili\n",
    "\n",
    "* **Filtri per documento/lingua**: aggiungi `query_filter=Filter(must=[FieldCondition(key=\"doc_id\", match=MatchValue(value=\"...\"))])` nella `query_points` per limitare a un documento o lingua.\n",
    "* **Score threshold**: dopo la ricerca, scarta risultati con score semantico basso (es. `< 0.3` normalizzato) per ridurre rumore.\n",
    "* **Re-ingest**: per aggiornare un documento, cancella i vecchi chunk per `doc_id` e re-inserisci.\n",
    "* **Cross-encoder reranking**: per qualità top-K ancora migliore, applica un re-rank con un modello tipo `ms-marco` su 30–50 candidati.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Checklist rapida\n",
    "\n",
    "* [ ] Qdrant avviato con storage persistente\n",
    "* [ ] Embedding corretto (dimensione = `VectorParams.size`)\n",
    "* [ ] Collection con **COSINE** + HNSW (`m`, `ef_construct`)\n",
    "* [ ] `SearchParams(hnsw_ef, exact)` tarati per recall/latency\n",
    "* [ ] Indici payload: **TEXT** per `text`, **KEYWORD** per metadati\n",
    "* [ ] Ingest chunk → embed → upsert (`wait=True`)\n",
    "* [ ] **Hybrid**: semantica + pre-filtro testuale + fusione (+ MMR)\n",
    "* [ ] Prompt con **citazioni** e regole anti-hallucination\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ca7a0-6cde-4f96-a5aa-4ae504d4e2d1",
   "metadata": {},
   "source": [
    "# Cypher (Neo4j) \n",
    "\n",
    "## 1) Concetti base del modello a grafo\n",
    "\n",
    "* **Nodo**: un’entità (persona, azienda, prodotto). Può avere **etichette** (labels) e **proprietà** (chiave→valore).\n",
    "* **Relazione**: collega due nodi, ha **direzione**, **tipo** (es. `:WORKS_FOR`) e può avere **proprietà**.\n",
    "* **Pattern**: combinazioni di nodi e relazioni che descrivono ciò che vuoi creare, cercare, aggiornare o cancellare.\n",
    "\n",
    "## 2) Sintassi “ASCII-art” di Cypher\n",
    "\n",
    "* Nodo: `(n:Label {prop: \"val\"})`\n",
    "* Relazione direzionata: `()-[:REL]->()`\n",
    "* Relazione indifferente alla direzione: `()-[:REL]-()`\n",
    "* Esempio visivo:\n",
    "  `(:Person {name:\"Sally\"})-[:LIKES]->(:Topic {name:\"Graphs\"})`\n",
    "\n",
    "## 3) Pattern matching: come “raccontare” il grafo\n",
    "\n",
    "In inglese: “Sally likes Graphs. Sally is friends with John. Sally works for Neo4j.”\n",
    "In Cypher come pattern:\n",
    "\n",
    "```cypher\n",
    "(:Person {name:\"Sally\"})-[:LIKES]->(:Topic {name:\"Graphs\"})\n",
    "(:Person {name:\"Sally\"})-[:IS_FRIENDS_WITH]->(:Person {name:\"John\"})\n",
    "(:Person {name:\"Sally\"})-[:WORKS_FOR]->(:Company {name:\"Neo4j\"})\n",
    "```\n",
    "\n",
    "Per usarli davvero devi **creare** prima i nodi e le relazioni (vedi sezione 4).\n",
    "\n",
    "## 4) CRUD essenziale con Cypher\n",
    "\n",
    "### CREATE — crea nodi/relazioni\n",
    "\n",
    "```cypher\n",
    "CREATE (s:Sally {name:\"Sally\"})\n",
    "CREATE (g:Graphs {name:\"Graphs\"})\n",
    "CREATE (j:John {name:\"John\"})\n",
    "CREATE (n:Neo4j {name:\"Neo4j\"})\n",
    "\n",
    "CREATE (s)-[:LIKES]->(g)\n",
    "CREATE (s)-[:IS_FRIENDS_WITH]->(j)\n",
    "CREATE (s)-[:WORKS_FOR]->(n);\n",
    "```\n",
    "\n",
    "> Nota: qui ho usato etichette “personali” (`:Sally`, `:John`…) per restare aderente all’esempio testuale. In pratica è meglio usare etichette **semantiche** come `:Person`, `:Company`, `:Topic` e spostare i nomi in proprietà (`name`).\n",
    "\n",
    "**Versione “buona pratica”:**\n",
    "\n",
    "```cypher\n",
    "CREATE (s:Person {name:\"Sally\"})\n",
    "CREATE (g:Topic  {name:\"Graphs\"})\n",
    "CREATE (j:Person {name:\"John\"})\n",
    "CREATE (c:Company {name:\"Neo4j\"})\n",
    "\n",
    "CREATE (s)-[:LIKES]->(g)\n",
    "CREATE (s)-[:IS_FRIENDS_WITH]->(j)\n",
    "CREATE (s)-[:WORKS_FOR]->(c);\n",
    "```\n",
    "\n",
    "### MATCH / RETURN — leggi (query)\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Person {name:\"Sally\"})-[:LIKES]->(t:Topic)\n",
    "RETURN p.name AS person, t.name AS topic;\n",
    "```\n",
    "\n",
    "### WHERE — filtri\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Person)-[:IS_FRIENDS_WITH]->(f:Person)\n",
    "WHERE p.name = \"Sally\" AND f.name STARTS WITH \"J\"\n",
    "RETURN f.name;\n",
    "```\n",
    "\n",
    "### SET — aggiorna proprietà/etichette\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Person {name:\"Sally\"})\n",
    "SET p.role = \"Engineer\", p.since = date(\"2023-01-01\");\n",
    "```\n",
    "\n",
    "### DELETE / DETACH DELETE — cancella\n",
    "\n",
    "```cypher\n",
    "MATCH (t:Topic {name:\"Graphs\"}) DETACH DELETE t;  // cancella nodo e le sue relazioni\n",
    "```\n",
    "\n",
    "## 5) MERGE vs CREATE (idempotenza)\n",
    "\n",
    "* **CREATE** crea sempre (può produrre duplicati).\n",
    "* **MERGE** “trova-o-crea”: se il pattern non esiste, lo crea; altrimenti lo riusa.\n",
    "\n",
    "```cypher\n",
    "MERGE (s:Person {name:\"Sally\"})\n",
    "MERGE (c:Company {name:\"Neo4j\"})\n",
    "MERGE (s)-[r:WORKS_FOR]->(c)\n",
    "ON CREATE SET r.since = date()     // eseguito solo se la relazione è appena creata\n",
    "ON MATCH  SET r.lastSeen = date(); // eseguito se esisteva già\n",
    "```\n",
    "\n",
    "## 6) Variabili, labels e proprietà (pratica corretta)\n",
    "\n",
    "* **Variabili**: nomi tra parentesi per riusare i nodi/relazioni nel resto della query (es. `(p:Person)`).\n",
    "* **Più label**: `(p:Person:Employee)` per categorizzare allo stesso tempo.\n",
    "* **Proprietà**: tipi primitivi (stringhe, numeri, booleani, date, liste, mappe).\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Person {name:$name})-[:WORKS_FOR]->(c:Company)\n",
    "RETURN p{.*, labels:labels(p)} AS person, c.name AS company;\n",
    "```\n",
    "\n",
    "> `$name` è un **parametro** (vedi §8).\n",
    "\n",
    "## 7) Direzione e tipo di relazione\n",
    "\n",
    "* La direzione conta quando la semantica è asimmetrica (es. `WORKS_FOR`).\n",
    "* Se non ti interessa la direzione, usa un trattino semplice su entrambi i lati: `()-[:REL]-()`.\n",
    "\n",
    "Esempi:\n",
    "\n",
    "```cypher\n",
    "// Direzionata: chi lavora per cosa\n",
    "MATCH (p:Person)-[:WORKS_FOR]->(c:Company) RETURN p, c;\n",
    "\n",
    "// Indifferente (amicizia bidirezionale modellata come relazione singola)\n",
    "MATCH (a:Person)-[:IS_FRIENDS_WITH]-(b:Person) RETURN a, b;\n",
    "```\n",
    "\n",
    "## 8) Parametri (sicurezza e performance)\n",
    "\n",
    "Evita di interpolare stringhe; usa parametri:\n",
    "\n",
    "```cypher\n",
    ":param name => \"Sally\";\n",
    "\n",
    "MATCH (p:Person {name:$name})-[:LIKES]->(t:Topic)\n",
    "RETURN t.name;\n",
    "```\n",
    "\n",
    "Vantaggi: prevenzione di injection e caching dei piani di esecuzione.\n",
    "\n",
    "## 9) Indici e vincoli (per velocità e qualità dati)\n",
    "\n",
    "* **Index**: velocizza `MATCH` su proprietà usate nei filtri.\n",
    "* **Unique constraint**: impedisce duplicati su una chiave naturale (es. `Person(name)` se unico).\n",
    "\n",
    "Esempi:\n",
    "\n",
    "```cypher\n",
    "// Index (Neo4j 5+)\n",
    "CREATE INDEX person_name IF NOT EXISTS FOR (p:Person) ON (p.name);\n",
    "\n",
    "// Unique constraint\n",
    "CREATE CONSTRAINT unique_company_name IF NOT EXISTS\n",
    "FOR (c:Company) REQUIRE c.name IS UNIQUE;\n",
    "```\n",
    "\n",
    "## 10) “Come funziona” in pratica (schema mentale)\n",
    "\n",
    "1. **Scrivi un pattern** che descriva ciò che cerchi/crei.\n",
    "2. **MATCH** (leggi) o **CREATE/MERGE** (scrivi) su quel pattern.\n",
    "3. Applica **WHERE** per filtrare.\n",
    "4. **RETURN** per restituire dati (o **SET/DELETE** per modificare).\n",
    "5. Usa **parametri, indici e vincoli** per robustezza e performance.\n",
    "\n",
    "## 11) Mini-laboratorio (copiabile)\n",
    "\n",
    "**A. Crea il grafo “Sally” in modo corretto**\n",
    "\n",
    "```cypher\n",
    "MERGE (s:Person  {name:\"Sally\"})\n",
    "MERGE (j:Person  {name:\"John\"})\n",
    "MERGE (g:Topic   {name:\"Graphs\"})\n",
    "MERGE (c:Company {name:\"Neo4j\"})\n",
    "\n",
    "MERGE (s)-[:LIKES]->(g)\n",
    "MERGE (s)-[:IS_FRIENDS_WITH]->(j)\n",
    "MERGE (s)-[:WORKS_FOR]->(c);\n",
    "```\n",
    "\n",
    "**B. Trova ciò che piace a Sally**\n",
    "\n",
    "```cypher\n",
    "MATCH (s:Person {name:\"Sally\"})-[:LIKES]->(t:Topic)\n",
    "RETURN t.name AS topic;\n",
    "```\n",
    "\n",
    "**C. Trova gli amici di Sally che lavorano da qualche parte**\n",
    "\n",
    "```cypher\n",
    "MATCH (s:Person {name:\"Sally\"})-[:IS_FRIENDS_WITH]-(f:Person)-[:WORKS_FOR]->(c:Company)\n",
    "RETURN f.name AS friend, c.name AS company;\n",
    "```\n",
    "\n",
    "**D. Aggiorna un attributo**\n",
    "\n",
    "```cypher\n",
    "MATCH (s:Person {name:\"Sally\"})-[:WORKS_FOR]->(c:Company {name:\"Neo4j\"})\n",
    "SET s.role = \"Engineer\", s.seniority = \"Mid\"\n",
    "RETURN s.name, s.role, s.seniority;\n",
    "```\n",
    "\n",
    "**E. Cancella un topic e le sue relazioni**\n",
    "\n",
    "```cypher\n",
    "MATCH (t:Topic {name:\"Graphs\"}) DETACH DELETE t;\n",
    "```\n",
    "\n",
    "## 12) Errori comuni da evitare\n",
    "\n",
    "* **Etichette come nomi propri** (`:Sally`) invece di etichette semantiche (`:Person`) → usa proprietà per i nomi.\n",
    "* **Duplicati con CREATE** → preferisci **MERGE** quando vuoi idempotenza.\n",
    "* **Dimenticare vincoli** → rischio dati sporchi; crea **unique constraints** per chiavi naturali.\n",
    "* **WHERE sul pattern sbagliato** → assicurati che le variabili del `WHERE` siano definite nel `MATCH` precedente.\n",
    "* **Direzione errata** → controlla la freccia delle relazioni in base alla semantica.\n",
    "\n",
    "## 13) Esercizi rapidi\n",
    "\n",
    "1. Aggiungi il topic `:Topic {name:\"Cypher\"}` e collega Sally con `[:LIKES]`.\n",
    "2. Trova tutti i `(p:Person)` che **non** lavorano per nessuna `:Company`.\n",
    "3. Impedisci duplicati su `Person(name)` con un vincolo unico e verifica l’errore provando a reinserire “Sally”.\n",
    "4. Trasforma `IS_FRIENDS_WITH` in una relazione **indifferente alla direzione** nelle query.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866813a-aa1c-4388-8a99-75d6097a77ec",
   "metadata": {},
   "source": [
    "![Alt text](sql-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56609e5e-d418-4338-8a64-ec51426086ed",
   "metadata": {},
   "source": [
    "# Confronto **Cypher vs SQL** con dataset Northwind\n",
    "# https://github.com/neo4j-graph-examples/northwind\n",
    "\n",
    "\n",
    "Cypher (linguaggio per database a grafo come Neo4j) e SQL (linguaggio per database relazionali) risolvono problemi simili (query, inserimenti, aggiornamenti, aggregazioni), ma con **paradigmi diversi**:\n",
    "\n",
    "* SQL lavora su **tabelle relazionali** e fa uso di **JOIN**.\n",
    "* Cypher lavora su **nodi e relazioni** e usa il **pattern matching**.\n",
    "\n",
    "Il dataset **Northwind** è un esempio utile per confrontare i due mondi.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Indexing\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "CREATE INDEX Product_productName ON products (product_name);\n",
    "CREATE INDEX Product_unitPrice ON products (unit_price);\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "CREATE INDEX Product_productName IF NOT EXISTS FOR (p:Product) ON (p.productName);\n",
    "CREATE INDEX Product_unitPrice IF NOT EXISTS FOR (p:Product) ON (p.unitPrice);\n",
    "```\n",
    "\n",
    "**Differenza chiave**:\n",
    "\n",
    "* SQL usa l’indice in tutte le fasi di query.\n",
    "* Cypher usa l’indice solo per **trovare i nodi iniziali**, poi segue i legami del grafo.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Select e Return di tutti i record\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT p.*\n",
    "FROM products AS p;\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product)\n",
    "RETURN p;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Field access, ordinamento e paging\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT p.ProductName, p.UnitPrice\n",
    "FROM products AS p\n",
    "ORDER BY p.UnitPrice DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product)\n",
    "RETURN p.productName, p.unitPrice\n",
    "ORDER BY p.unitPrice DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Trova un prodotto per nome\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT p.ProductName, p.UnitPrice\n",
    "FROM products AS p\n",
    "WHERE p.ProductName = 'Chocolade';\n",
    "```\n",
    "\n",
    "**Cypher** (due alternative)\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product)\n",
    "WHERE p.productName = 'Chocolade'\n",
    "RETURN p.productName, p.unitPrice;\n",
    "\n",
    "MATCH (p:Product {productName:'Chocolade'})\n",
    "RETURN p.productName, p.unitPrice;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Filtrare prodotti\n",
    "\n",
    "### IN / lista valori\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT p.ProductName, p.UnitPrice\n",
    "FROM products AS p\n",
    "WHERE p.ProductName IN ('Chocolade','Chai');\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product)\n",
    "WHERE p.productName IN ['Chocolade','Chai']\n",
    "RETURN p.productName, p.unitPrice;\n",
    "```\n",
    "\n",
    "### Predicati multipli\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT p.ProductName, p.UnitPrice\n",
    "FROM products AS p\n",
    "WHERE p.ProductName LIKE 'C%' AND p.UnitPrice > 100;\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product)\n",
    "WHERE p.productName STARTS WITH 'C' AND p.unitPrice > 100\n",
    "RETURN p.productName, p.unitPrice;\n",
    "\n",
    "MATCH (p:Product)\n",
    "WHERE p.productName =~ '^C.*'\n",
    "RETURN p.productName, p.unitPrice;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Join di più tabelle vs pattern\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT DISTINCT c.CompanyName\n",
    "FROM customers AS c\n",
    "JOIN orders AS o ON (c.CustomerID = o.CustomerID)\n",
    "JOIN order_details AS od ON (o.OrderID = od.OrderID)\n",
    "JOIN products AS p ON (od.ProductID = p.ProductID)\n",
    "WHERE p.ProductName = 'Chocolade';\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (p:Product {productName:'Chocolade'})<-[:ORDERS]-(:Order)<-[:PURCHASED]-(c:Customer)\n",
    "RETURN DISTINCT c.companyName;\n",
    "```\n",
    "\n",
    "**Differenza chiave**:\n",
    "\n",
    "* SQL richiede JOIN espliciti.\n",
    "* Cypher usa direttamente i **legami del grafo**, più intuitivi da leggere.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Aggregazioni e SUM\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT p.Product_Name, sum(od.Unit_Price * od.Quantity) AS TotalPrice\n",
    "FROM customers AS c\n",
    "LEFT OUTER JOIN orders AS o ON (c.Customer_ID = o.Customer_ID)\n",
    "LEFT OUTER JOIN order_details AS od ON (o.Order_ID = od.Order_ID)\n",
    "LEFT OUTER JOIN products AS p ON (od.Product_ID = p.Product_ID)\n",
    "WHERE c.Company_Name = 'Drachenblut Delikatessen'\n",
    "GROUP BY p.Product_Name;\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (c:Customer {companyName:'Drachenblut Delikatessen'})\n",
    "OPTIONAL MATCH (c)-[:PURCHASED]->(:Order)-[o:ORDERS]->(p:Product)\n",
    "RETURN p.productName, toInteger(sum(o.unitPrice * o.quantity)) AS totalPrice;\n",
    "```\n",
    "\n",
    "**Nota**: in Cypher non serve `GROUP BY` esplicito: appena usi una funzione di aggregazione, le altre variabili diventano automaticamente grouping keys.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) COUNT dei prodotti forniti\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT s.CompanyName AS Supplier, COUNT(p.ProductID) AS NumberOfProducts\n",
    "FROM Suppliers s\n",
    "JOIN Products p ON s.SupplierID = p.SupplierID\n",
    "GROUP BY s.CompanyName\n",
    "ORDER BY NumberOfProducts DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (s:Supplier)<-[:SUPPLIED_BY]-(p:Product)\n",
    "RETURN s.companyName AS Supplier, COUNT(p) AS NumberOfProducts\n",
    "ORDER BY NumberOfProducts DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9) COLLECT / STRING\\_AGG (liste)\n",
    "\n",
    "**SQL**\n",
    "\n",
    "```sql\n",
    "SELECT s.CompanyName AS Supplier,\n",
    "       STRING_AGG(p.ProductName, ', ' ORDER BY p.ProductName) AS ProductsSupplied\n",
    "FROM Suppliers s\n",
    "JOIN Products p ON s.SupplierID = p.SupplierID\n",
    "GROUP BY s.CompanyName\n",
    "ORDER BY s.CompanyName\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "**Cypher**\n",
    "\n",
    "```cypher\n",
    "MATCH (s:Supplier)-[:SUPPLIES]->(p:Product)\n",
    "RETURN s.companyName AS Supplier, COLLECT(p.productName) AS ProductsSupplied\n",
    "ORDER BY Supplier\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Differenze concettuali riassunte\n",
    "\n",
    "* **Join vs Pattern Matching**\n",
    "\n",
    "  * SQL: `JOIN` espliciti.\n",
    "  * Cypher: relazioni dirette `()-[:REL]->()`.\n",
    "\n",
    "* **Aggregazioni**\n",
    "\n",
    "  * SQL: richiede `GROUP BY`.\n",
    "  * Cypher: grouping implicito appena usi funzioni come `COUNT`, `SUM`, `COLLECT`.\n",
    "\n",
    "* **Indice**\n",
    "\n",
    "  * SQL: usato in tutto il piano di esecuzione.\n",
    "  * Cypher: serve solo per trovare nodi di partenza, poi segue le relazioni.\n",
    "\n",
    "* **Espressività**\n",
    "\n",
    "  * SQL: ottimo su dati tabellari.\n",
    "  * Cypher: intuitivo e più vicino al linguaggio naturale quando si lavora con reti di entità collegate.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
